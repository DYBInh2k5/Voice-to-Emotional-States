# ğŸ¤ Voice to Emotional States

A comprehensive machine learning project for recognizing emotional states from voice audio using advanced audio processing and classification techniques.

## ğŸŒŸ Features

- **Audio Feature Extraction**: MFCC, spectral features, chroma, zero-crossing rate
- **Multi-Emotion Classification**: 7 emotion categories (neutral, happy, sad, angry, fear, disgust, surprise)
- **Real-time Detection**: Live emotion recognition from microphone input
- **Command-Line Interface**: Easy-to-use CLI for training and prediction
- **Demo Mode**: Quick testing with synthetic data
- **Jupyter Integration**: Interactive notebooks for experimentation
- **Multiple Audio Formats**: Support for WAV, MP3, FLAC files

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/DYBInh2k5/Voice-to-Emotional-States.git
cd Voice-to-Emotional-States

# Install dependencies
pip install -r requirements.txt
```

### Run Demo

```bash
# Quick demo with synthetic data
python demo.py

# Create test audio files
python create_test_audio.py

# Test prediction
python main.py predict --audio test_audio/happy_test.wav --model models/demo_emotion_model.pkl
```

## ğŸ’» Usage

### Command Line Interface

```bash
# Train a new model
python main.py train --data path/to/dataset --output models/my_model.pkl

# Predict emotion from audio file
python main.py predict --audio audio_file.wav --model models/trained_model.pkl

# Real-time emotion detection
python main.py realtime --model models/trained_model.pkl

# Show help
python main.py --help
```

### Python API

```python
from src.voice_emotion import VoiceEmotionAnalyzer

# Load trained model
analyzer = VoiceEmotionAnalyzer('models/demo_emotion_model.pkl')

# Predict emotion
result = analyzer.predict_emotion("audio_file.wav")
print(f"Emotion: {result['emotion']}")
print(f"Confidence: {result['confidence']:.2f}")
```

### Real-time Detection

```python
from src.real_time_detector import RealTimeEmotionDetector

# Start real-time detection
detector = RealTimeEmotionDetector('models/demo_emotion_model.pkl')
detector.start_recording()  # Press Ctrl+C to stop
```

## ğŸ“ Project Structure

```
Voice-to-Emotional-States/
â”œâ”€â”€ src/                          # Source code
â”‚   â”œâ”€â”€ voice_emotion.py         # Core emotion analyzer
â”‚   â”œâ”€â”€ data_processor.py        # Dataset processing utilities
â”‚   â”œâ”€â”€ real_time_detector.py    # Real-time detection
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ models/                       # Trained models
â”‚   â””â”€â”€ demo_emotion_model.pkl   # Pre-trained demo model
â”œâ”€â”€ test_audio/                   # Test audio files
â”‚   â”œâ”€â”€ happy_test.wav
â”‚   â”œâ”€â”€ sad_test.wav
â”‚   â”œâ”€â”€ angry_test.wav
â”‚   â””â”€â”€ neutral_test.wav
â”œâ”€â”€ notebooks/                    # Jupyter notebooks
â”‚   â””â”€â”€ emotion_analysis.ipynb   # Analysis and experimentation
â”œâ”€â”€ main.py                      # CLI interface
â”œâ”€â”€ demo.py                      # Demo script
â”œâ”€â”€ create_test_audio.py         # Generate test audio
â”œâ”€â”€ project_summary.py           # Project overview
â”œâ”€â”€ requirements.txt             # Dependencies
â”œâ”€â”€ .gitignore                   # Git ignore rules
â””â”€â”€ README.md                    # This file
```

## ğŸ˜Š Supported Emotions

| Emotion | Description |
|---------|-------------|
| ğŸ˜ Neutral | Calm, balanced emotional state |
| ğŸ˜Š Happy | Joy, excitement, positive emotions |
| ğŸ˜¢ Sad | Sorrow, melancholy, low energy |
| ğŸ˜  Angry | Frustration, irritation, high energy |
| ğŸ˜¨ Fear | Anxiety, worry, nervousness |
| ğŸ¤¢ Disgust | Aversion, repulsion |
| ğŸ˜² Surprise | Shock, amazement, unexpected |

## ğŸ”§ Technical Details

### Audio Features
- **MFCC (Mel-Frequency Cepstral Coefficients)**: Captures spectral characteristics
- **Spectral Centroid**: Measures brightness of sound
- **Zero Crossing Rate**: Indicates voicing characteristics
- **Chroma Features**: Represents pitch class profiles
- **Mel Spectrogram**: Time-frequency representation

### Machine Learning
- **Algorithm**: Random Forest Classifier
- **Feature Scaling**: StandardScaler normalization
- **Training Data**: Synthetic data with emotion-specific patterns
- **Validation**: Cross-validation and confidence scoring

## ğŸ“Š Performance

The demo model achieves reasonable accuracy on synthetic data. For production use:
- Collect real audio datasets (RAVDESS, CREMA-D, etc.)
- Implement data augmentation techniques
- Experiment with deep learning models (CNN, RNN, Transformers)
- Fine-tune hyperparameters

## ğŸ› ï¸ Development

### Running Tests
```bash
python test_all.py
```

### Jupyter Notebook
```bash
jupyter notebook notebooks/emotion_analysis.ipynb
```

### Project Summary
```bash
python project_summary.py
```

## ğŸ“¦ Dependencies

- **numpy**: Numerical computing
- **pandas**: Data manipulation
- **scikit-learn**: Machine learning algorithms
- **librosa**: Audio processing
- **tensorflow**: Deep learning (optional)
- **matplotlib/seaborn**: Visualization
- **soundfile**: Audio I/O
- **pyaudio**: Real-time audio capture
- **jupyter**: Interactive notebooks

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **librosa** team for excellent audio processing tools
- **scikit-learn** for machine learning algorithms
- Audio dataset providers (RAVDESS, CREMA-D, etc.)
- Open source community for inspiration and tools

## ğŸ“ Contact

- **Author**: DYBInh2k5
- **GitHub**: [@DYBInh2k5](https://github.com/DYBInh2k5)
- **Repository**: [Voice-to-Emotional-States](https://github.com/DYBInh2k5/Voice-to-Emotional-States)

---

â­ **Star this repository if you found it helpful!** â­